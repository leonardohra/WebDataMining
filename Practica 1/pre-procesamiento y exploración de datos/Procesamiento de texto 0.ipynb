{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ciclo de Minería de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ciclo General de KDD](ciclo-kdd.png \"Ciclo General de KDD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo del preprocesamiento es pasar de un _trozo de texto_ (documento, página web, tweet, etc), es decir, una cadena única larga y sin procesar, a una lista (o varias listas) de **tokens** limpios que sean útiles para tareas adicionales de minería de textos y/o procesamiento de lenguaje natural.\n",
    "\n",
    "Este proceso tiene fundamentalmente tres etapas:\n",
    "\n",
    "![Preprocesamiento de texto](ciclo-nlp.png \"Preprocesamiento de texto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Eliminación de ruido\n",
    "Son tareas que normalmente se realizan antes de la tokenización. La eliminación de ruido es más específica del dominio que las otras dos etapas (tokenización y normalización)\n",
    "\n",
    "Las tareas de eliminación de ruido incluye:\n",
    "\n",
    "* eliminar encabezados de archivos de texto, pies de página\n",
    "* eliminar marcas HTML, XML, etc. y metadatos\n",
    "* extraer datos valiosos de otros formatos, como JSON\n",
    "\n",
    "Veamos un ejempolo de eliminación de etiquetas HTML con la ayuda de la biblioteca BeautifulSoup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsear HTML con BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c anaconda beautifulsoup4 \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos crear un objeto a partir de un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head><title>Webmining course</title></head>\n",
      "<body>\n",
      "<p class=\"title\"><b>Three Little Pigs</b></p>\n",
      "<p class=\"story\">Once upon a time there were three little pigs; and their names were\n",
      "<a class=\"pig\" href=\"http://example.com/Browny\" id=\"link1\">Browny</a>,\n",
      "<a class=\"pig\" href=\"http://example.com/Whitey\" id=\"link2\">Whitey</a> and\n",
      "<a class=\"pig\" href=\"http://example.com/Blacky\" id=\"link3\">Blacky</a>;\n",
      "and they lived in a small farm.</p>\n",
      "<p class=\"story\">...</p></body></html>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"three-little-pigs.html\") as fp:\n",
    "    soup = BeautifulSoup(fp)\n",
    "print(soup)\n",
    "type(soup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O a partir de un string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<html><head><title>Webmining course</title></head>\n",
    "<body>\n",
    "<p class=\"main-title\" id=\"title\"><b>Three Little Pigs</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little pigs; and their names were\n",
    "<a href=\"http://example.com/Browny\" class=\"pig\" id=\"link1\">Browny</a>,\n",
    "<a href=\"http://example.com/Whitey\" class=\"pig\" id=\"link2\">Whitey</a> and\n",
    "<a href=\"http://example.com/Blacky\" class=\"pig\" id=\"link3\">Blacky</a>;\n",
    "and they lived in a small farm.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al crear un objeto BeautifulSoup, obtenemos la estructura anidada del HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Webmining course\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"main-title\" id=\"title\">\n",
      "   <b>\n",
      "    Three Little Pigs\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   Once upon a time there were three little pigs; and their names were\n",
      "   <a class=\"pig\" href=\"http://example.com/Browny\" id=\"link1\">\n",
      "    Browny\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"pig\" href=\"http://example.com/Whitey\" id=\"link2\">\n",
      "    Whitey\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"pig\" href=\"http://example.com/Blacky\" id=\"link3\">\n",
      "    Blacky\n",
      "   </a>\n",
      "   ;\n",
      "and they lived in a small farm.\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   ...\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen varias formas de navegar por esta estructura, por ejemplo, accediendo a los tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Webmining course</title>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'title'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Webmining course'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'head'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"main-title\" id=\"title\"><b>Three Little Pigs</b></p>\n"
     ]
    }
   ],
   "source": [
    "first_p = soup.p\n",
    "print(first_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pueden listar los atributos de un tag y acceder a sus atributos usando corchetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class': ['main-title'], 'id': 'title'}\n",
      "['main-title']\n"
     ]
    }
   ],
   "source": [
    "print(first_p.attrs)\n",
    "print(first_p['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es posible cambiar un tag directamente sobre el objeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 class=\"main-title\" id=\"title\"><b>Three Little Pigs</b></h1>\n"
     ]
    }
   ],
   "source": [
    "first_p.name = \"h1\"\n",
    "print(first_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y agregar, eliminar y editar atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 another-attribute=\"1\" class=\"main-title\" id=\"verybold\"><b>Three Little Pigs</b></h1>\n"
     ]
    }
   ],
   "source": [
    "first_p['id'] = 'verybold'\n",
    "first_p['another-attribute'] = 1\n",
    "print(first_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 class=\"main-title\"><b>Three Little Pigs</b></h1>\n"
     ]
    }
   ],
   "source": [
    "del first_p['id']\n",
    "del first_p['another-attribute']\n",
    "print(first_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-0e94b8d54f2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfirst_p\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#Debería dar un error indicando que first_p no tiene un atributo id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \"\"\"tag[key] returns the value of the 'key' attribute for the tag,\n\u001b[0;32m   1015\u001b[0m         and throws an exception if it's not there.\"\"\"\n\u001b[1;32m-> 1016\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1017\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "first_p['id'] #Debería dar un error indicando que first_p no tiene un atributo id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(first_p.get('id')) \n",
    "#No da error, sino un \"None\". \n",
    "#En python este valor indica que una variable no tiene valor, \n",
    "#es equivalente al null de java y otros lenguajes, pero ES un objeto y se comporta como tal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los links se obtienen de forma similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"pig\" href=\"http://example.com/Browny\" id=\"link1\">Browny</a>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"pig\" href=\"http://example.com/Browny\" id=\"link1\">Browny</a>,\n",
       " <a class=\"pig\" href=\"http://example.com/Whitey\" id=\"link2\">Whitey</a>,\n",
       " <a class=\"pig\" href=\"http://example.com/Blacky\" id=\"link3\">Blacky</a>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"pig\" href=\"http://example.com/Browny\" id=\"link1\">Browny</a>,\n",
       " <a class=\"pig\" href=\"http://example.com/Whitey\" id=\"link2\">Whitey</a>,\n",
       " <a class=\"pig\" href=\"http://example.com/Blacky\" id=\"link3\">Blacky</a>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"a\", class_=\"pig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"pig\" href=\"http://example.com/Browny\" id=\"link1\">Browny</a>,\n",
       " <a class=\"pig\" href=\"http://example.com/Whitey\" id=\"link2\">Whitey</a>]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"a\", limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"pig\" href=\"http://example.com/Blacky\" id=\"link3\">Blacky</a>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(id=\"link3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algo que podríamos necesitar es obtener todos los links (url) del documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://example.com/Browny\n",
      "http://example.com/Whitey\n",
      "http://example.com/Blacky\n"
     ]
    }
   ],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los hijos de un tag tag’s pueden accederse utilizando **.contents**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tag head es:  <head><title>Webmining course</title></head>\n",
      "Los hijos del tag head son:  [<title>Webmining course</title>]\n",
      "El primer hijo del tag head es:  <title>Webmining course</title>\n",
      "Los hijos del primer hijo del tag head son:  ['Webmining course']\n"
     ]
    }
   ],
   "source": [
    "head_tag = soup.head\n",
    "print(\"El tag head es: \",head_tag)\n",
    "\n",
    "print(\"Los hijos del tag head son: \",head_tag.contents)\n",
    "\n",
    "title_tag = head_tag.contents[0]\n",
    "print(\"El primer hijo del tag head es: \", title_tag)\n",
    "\n",
    "print(\"Los hijos del primer hijo del tag head son: \",title_tag.contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En vez de obtenerlos como una lista, podemos iterar sobre los hijos usando el generador **.children**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webmining course\n"
     ]
    }
   ],
   "source": [
    "for child in title_tag.children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los atributos **.contents** y **.children** solo consideran los hijos directos de un tag. Por ejemplo, el tag **head** solo tiene un hijo directo **title**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>Webmining course</title>]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_tag.contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, el tag **title** también tiene un hijo: el string \"Webmining course\". El atributo **.descendants** permite iterar sobre todos los hijos (directos o no) de un tag, recursivamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Webmining course</title>\n",
      "Webmining course\n"
     ]
    }
   ],
   "source": [
    "for child in head_tag.descendants:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos también pedir todos los strings de un documento, pero vamos a observar que no nos retorna solo el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n'\n",
      "'Webmining course'\n",
      "'\\n'\n",
      "'\\n'\n",
      "'Three Little Pigs'\n",
      "'\\n'\n",
      "'Once upon a time there were three little pigs; and their names were\\n'\n",
      "'Browny'\n",
      "',\\n'\n",
      "'Whitey'\n",
      "' and\\n'\n",
      "'Blacky'\n",
      "';\\nand they lived in a small farm.'\n",
      "'\\n'\n",
      "'...'\n",
      "'\\n'\n"
     ]
    }
   ],
   "source": [
    "for string in soup.strings:\n",
    "    print(repr(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pueden eliminar todos los \"espacios\" extra, utilizando **.stripped_strings**\n",
    "Con este atributo, se ignoran todos los strings que consisten solo de espacios y se eliminan los espacios al principio y al final de cada string. (notar que en la anteúltima linea queda sin eliminar un \"\\n\" porque no está al principio del string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Webmining course'\n",
      "'Three Little Pigs'\n",
      "'Once upon a time there were three little pigs; and their names were'\n",
      "'Browny'\n",
      "','\n",
      "'Whitey'\n",
      "'and'\n",
      "'Blacky'\n",
      "';\\nand they lived in a small farm.'\n",
      "'...'\n"
     ]
    }
   ],
   "source": [
    "for string in soup.stripped_strings:\n",
    "    print(repr(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra tarea común, es quedarnos con el texto, sin etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Webmining course\n",
      "\n",
      "Three Little Pigs\n",
      "Once upon a time there were three little pigs; and their names were\n",
      "Browny,\n",
      "Whitey and\n",
      "Blacky;\n",
      "and they lived in a small farm.\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma forma en la que se puede ir hacia abajo en la estructura, es posible navegar hacia arriba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El título es:  <title>Webmining course</title>\n",
      "y su padre es:  <head><title>Webmining course</title></head>\n"
     ]
    }
   ],
   "source": [
    "title_tag = soup.title\n",
    "print(\"El título es: \",title_tag)\n",
    "\n",
    "print(\"y su padre es: \",title_tag.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También se puede iterar sobre los padres de un tag con el atributo **.parents**. :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El primer link es:  <a class=\"pig\" href=\"http://example.com/Browny\" id=\"link1\">Browny</a>\n",
      "y sus padres son: \n",
      "p\n",
      "body\n",
      "html\n",
      "[document]\n"
     ]
    }
   ],
   "source": [
    "link = soup.a\n",
    "print(\"El primer link es: \",link)\n",
    "print(\"y sus padres son: \")\n",
    "for parent in link.parents:\n",
    "    print(parent.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expansión de contracciones en inglés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque es parte de la *normalización*, para el idioma inglés la expansión de contracciones debe realizarse **antes** de la tokenización. Esto es que el tokenizador dividirá palabras como “did not” en “did” y “not” y, si bien no es imposible remediar esta tokenización en una etapa posterior, hacerlo antes lo hace más fácil y directo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is an easy task. you are expecting to expand contractions\n",
      "do not do it, please just do not!\n"
     ]
    }
   ],
   "source": [
    "# pip install contractions\n",
    "# (no disponible desde anaconda)\n",
    "import contractions\n",
    "\n",
    "\n",
    "text = \"It's an easy task. You're expecting to expand contractions\\nDon't do it, please just don't!\"\n",
    "sample = contractions.fix(text)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Tokenización\n",
    "La tokenización es un paso que divide cadenas de texto en piezas más pequeñas o *tokens*. Los trozos de texto más grandes pueden ser convertidos en oraciones, las oraciones pueden ser divididas en palabras, etc. El procesamiento adicional generalmente se realiza después de que una pieza de texto ha sido apropiadamente concatenada. La tokenización también se conoce como *segmentación de texto* o *análisis léxico*. \n",
    "\n",
    "Si bien **segmentación** y **tokenización** a veces se utilizan indistintamente, siendo precisos **segmentación** se usa para referirse al desglose de un gran trozo de texto en partes más grandes que las palabras (por ejemplo, párrafos u oraciones), mientras que la tokenización se reserva para el proceso de desglose que se produce exclusivamente en palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "**NLTK** es un kit de herramientas de lenguaje natural y una de las bibliotecas NLP más conocidas y más utilizadas en el ecosistema de Python, útil para todo tipo de tareas, desde tokenización, hasta etiquetado de parte del habla (POS tagging), etc.\n",
    " - Es una biblioteca especializada en procesamiento de lenguaje natural\n",
    " - Recursos léxicos\n",
    " - Tokenización de palabras y oraciones\n",
    " - PoS Tagging\n",
    " - Identificación de entidades nombradas (NER)\n",
    " - Técnicas de minería de texto (por ejemplo clasificación)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conda install -c anaconda nltk \n",
    "\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separación de oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The quick brown fox jumed!', 'where?', 'Over the 2 lazy dogs.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "text = 'The quick brown fox jumed! where? Over the 2 lazy dogs.'\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separación de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumed', '!']\n",
      "['where', '?']\n",
      "['Over', 'the', '2', 'lazy', 'dogs', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumed', '!', 'where', '?', 'Over', 'the', '2', 'lazy', 'dogs', '.']\n"
     ]
    }
   ],
   "source": [
    "prueba = word_tokenize(text)\n",
    "print(prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciona para texto de redes sociales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'mg_armentano', ':', 'an', 'example', '!', ':', 'D', 'http', ':', '//example.com', '#', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "tweet = '@mg_armentano: an example! :D http://example.com #NLP'\n",
    "print(word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@mg_armentano', ':', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "tweet = '@mg_armentano: an example! :D http://example.com #NLP'\n",
    "print(tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textos de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      "<Text: Moby Dick by Herman Melville 1851>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "print(text1)\n",
    "type(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptualmente, un objeto **nltk.Text** no es más que una lista ordenada de tokens, siendo esto la unidad mínima de un texto, simplemente palabras o signos de puntuación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'Moby',\n",
       " 'Dick',\n",
       " 'by',\n",
       " 'Herman',\n",
       " 'Melville',\n",
       " '1851',\n",
       " ']',\n",
       " 'ETYMOLOGY',\n",
       " '.',\n",
       " '(',\n",
       " 'Supplied',\n",
       " 'by',\n",
       " 'a',\n",
       " 'Late',\n",
       " 'Consumptive',\n",
       " 'Usher',\n",
       " 'to',\n",
       " 'a',\n",
       " 'Grammar']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de textos desde url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "448614\n",
      "﻿Project Gutenberg's A Journey to the Interior of the Earth, by Jules Verne\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "\n",
    "url = \"http://www.gutenberg.org/cache/epub/3748/pg3748.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "print(type(raw))\n",
    "print(len(raw))\n",
    "print(raw[:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "91752\n",
      "['\\ufeffProject', 'Gutenberg', \"'s\", 'A', 'Journey', 'to', 'the', 'Interior', 'of', 'the']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "print(type(tokens))\n",
    "print(len(tokens))\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un objeto Text a partir de la secuencia de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.text.Text'>\n",
      "91752\n",
      "['\\ufeffProject', 'Gutenberg', \"'s\", 'A', 'Journey', 'to', 'the', 'Interior', 'of', 'the']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = nltk.Text(tokens)\n",
    "print(type(text))\n",
    "print(len(text))\n",
    "print(text[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\ufeffProject', 'Gutenberg'),\n",
       " ('Gutenberg', \"'s\"),\n",
       " (\"'s\", 'A'),\n",
       " ('A', 'Journey'),\n",
       " ('Journey', 'to'),\n",
       " ('to', 'the'),\n",
       " ('the', 'Interior'),\n",
       " ('Interior', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', 'Earth'),\n",
       " ('Earth', ','),\n",
       " (',', 'by'),\n",
       " ('by', 'Jules'),\n",
       " ('Jules', 'Verne'),\n",
       " ('Verne', 'This')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams(text))[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Normalización\n",
    "La normalización  se refiere a una serie de tareas destinadas a poner todo el texto en igualdad de condiciones: convirtiendo todo el texto a mayúsculas o minúsculas, eliminando la puntuación, convirtiendo los números a sus equivalentes de palabras. La normalización pone todas las palabras en pie de igualdad, y permite que el procesamiento proceda de manera uniforme.\n",
    "\n",
    "La normalización del texto puede significar realizar una serie de tareas, pero para nuestro marco abordaremos la normalización en 3 pasos distintos: (1) derivación, (2) lematización y (3) todo lo demás. \n",
    "\n",
    "Después de la tokenización, ya no estamos trabajando en un nivel de texto sino a nivel de palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminar caracteres no ASCII de una lista de palabras tokenizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffProject', 'Gutenberg', \"'s\", 'A', 'Journey', 'to', 'the', 'Interior', 'of', 'the', 'Earth', ',', 'by', 'Jules', 'Verne', 'This', 'eBook', 'is', 'for', 'the']\n",
      "['Project', 'Gutenberg', \"'s\", 'A', 'Journey', 'to', 'the', 'Interior', 'of', 'the', 'Earth', ',', 'by', 'Jules', 'Verne', 'This', 'eBook', 'is', 'for', 'the']\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "def remove_non_ascii(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "print(tokens[:20])\n",
    "tokens = remove_non_ascii(tokens)\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasar a minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', \"'s\", 'a', 'journey', 'to', 'the', 'interior', 'of', 'the', 'earth', ',', 'by', 'jules', 'verne', 'this', 'ebook', 'is', 'for', 'the']\n"
     ]
    }
   ],
   "source": [
    "def to_lowercase(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "tokens_low = to_lowercase(tokens)\n",
    "print(tokens_low[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminar signos de puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 's', 'a', 'journey', 'to', 'the', 'interior', 'of', 'the', 'earth', 'by', 'jules', 'verne', 'this', 'ebook', 'is', 'for', 'the', 'use']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "tokens_clean = remove_punctuation(tokens_low)\n",
    "print(tokens_clean[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Reemplazar números por su equivalente textual\n",
    "Usamos el módulo **inflect** que nos permite generar plurales, singulares, ordinales, artículos indefinidos y convertir números en palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'ran', 'one hundred', 'meters', 'and', 'rested', 'for', 'one', 'minute']\n"
     ]
    }
   ],
   "source": [
    "# conda install -c conda-forge inflect \n",
    "import inflect\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def replace_numbers(words):\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "example = \"She ran 100 meters and rested for 1 minute\"\n",
    "words = word_tokenize(example)\n",
    "words = replace_numbers(words)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminar stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'journey', 'interior', 'earth', 'jules', 'verne', 'ebook', 'use', 'anyone', 'anywhere', 'cost', 'almost', 'restrictions', 'whatsoever', 'may', 'copy', 'give', 'away', 'reuse']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "tokens_sin_stopwords = remove_stopwords(tokens_clean)\n",
    "print(tokens_sin_stopwords[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " 'gutenberg',\n",
       " 'journey',\n",
       " 'interior',\n",
       " 'earth',\n",
       " 'jules',\n",
       " 'verne',\n",
       " 'ebook',\n",
       " 'use',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'cost',\n",
       " 'almost',\n",
       " 'restrictions',\n",
       " 'whatsoever',\n",
       " 'may',\n",
       " 'copy',\n",
       " 'give',\n",
       " 'away',\n",
       " 'reuse']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokens_sin_stopwords)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " 'gutenberg',\n",
       " 'journey',\n",
       " 'interior',\n",
       " 'earth',\n",
       " 'jule',\n",
       " 'vern',\n",
       " 'ebook',\n",
       " 'use',\n",
       " 'anyon',\n",
       " 'anywher',\n",
       " 'cost',\n",
       " 'almost',\n",
       " 'restrict',\n",
       " 'whatsoev',\n",
       " 'may',\n",
       " 'copi',\n",
       " 'give',\n",
       " 'away',\n",
       " 'reus']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "list(porter.stem(t) for t in tokens_sin_stopwords)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " 'gutenberg',\n",
       " 'journey',\n",
       " 'intery',\n",
       " 'ear',\n",
       " 'jul',\n",
       " 'vern',\n",
       " 'ebook',\n",
       " 'us',\n",
       " 'anyon',\n",
       " 'anywh',\n",
       " 'cost',\n",
       " 'almost',\n",
       " 'restrict',\n",
       " 'whatsoev',\n",
       " 'may',\n",
       " 'cop',\n",
       " 'giv',\n",
       " 'away',\n",
       " 'reus']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "list(lancaster.stem(t) for t in tokens_sin_stopwords)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematización\n",
    "A diferencia del stemming, el lematizador solo elimina el final de la palabra si la palabra resultante se encuentra en el diccionario. Esto lo hace un proceso más lento que el stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " 'gutenberg',\n",
       " 'journey',\n",
       " 'interior',\n",
       " 'earth',\n",
       " 'jules',\n",
       " 'verne',\n",
       " 'ebook',\n",
       " 'use',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'cost',\n",
       " 'almost',\n",
       " 'restriction',\n",
       " 'whatsoever',\n",
       " 'may',\n",
       " 'copy',\n",
       " 'give',\n",
       " 'away',\n",
       " 'reuse']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "list(wnl.lemmatize(t) for t in tokens_sin_stopwords)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'visit', 'two', 'beauty', 'libr']\n",
      "['I', 'visited', 'two', 'beautiful', 'library']\n"
     ]
    }
   ],
   "source": [
    "t1 = \"I visited two beautiful libraries\"\n",
    "tokens_t = word_tokenize(t1)\n",
    "print(list(lancaster.stem(t) for t in tokens_t)[0:20])\n",
    "print(list(wnl.lemmatize(t) for t in tokens_t)[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploración del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algunas estadísticas del texto\n",
    "NLTK provee diferentes documentos de ejemplo. Podemos buscar calcular algunas estadísticas de algunos documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 25 26 austen-emma.txt\n",
      "5 26 17 austen-persuasion.txt\n",
      "5 28 22 austen-sense.txt\n",
      "4 34 79 bible-kjv.txt\n",
      "5 19 5 blake-poems.txt\n",
      "4 19 14 bryant-stories.txt\n",
      "4 18 12 burgess-busterbrown.txt\n",
      "4 20 13 carroll-alice.txt\n",
      "5 20 12 chesterton-ball.txt\n",
      "5 23 11 chesterton-brown.txt\n",
      "5 18 11 chesterton-thursday.txt\n",
      "4 21 25 edgeworth-parents.txt\n",
      "5 26 15 melville-moby_dick.txt\n",
      "5 52 11 milton-paradise.txt\n",
      "4 12 9 shakespeare-caesar.txt\n",
      "4 12 8 shakespeare-hamlet.txt\n",
      "4 12 7 shakespeare-macbeth.txt\n",
      "5 36 12 whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid))                              # raw() indica cuántas letras tiene el texto\n",
    "    num_words = len(gutenberg.words(fileid))                            # words() divide el texto en palabras\n",
    "    num_sents = len(gutenberg.sents(fileid))                            # sents() divide el texto en oraciones, donde cada oración es una lista de palabras\n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))    # contamos la cantidad de palabras únicas\n",
    "    print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)   \n",
    "    # promedio de longitud de las palabras, de las oraciones, cantidad de veces que cada palabra aparece en el texto en promedio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda con contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 455 matches:\n",
      " shall slay the dragon that is in the sea .\" -- ISAIAH \" And what thing soever \n",
      " S PLUTARCH ' S MORALS . \" The Indian Sea breedeth the most and the biggest fis\n",
      "cely had we proceeded two days on the sea , when about sunrise a great many Wha\n",
      "many Whales and other monsters of the sea , appeared . Among the former , one w\n",
      " waves on all sides , and beating the sea before him into a foam .\" -- TOOKE ' \n",
      "mediately lost and swallowed up , the sea - gudgeon retires into it in great se\n",
      "e .\" -- PILGRIM ' S PROGRESS . \" That sea beast Leviathan , which God of all hi\n",
      "s in , and at his breath spouts out a sea .\" -- IBID . \" The mighty whales whic\n",
      ". \" The mighty whales which swim in a sea of water , and have a sea of oil swim\n",
      "h swim in a sea of water , and have a sea of oil swimming in them .\" -- FULLLER\n",
      "RANS . A . D . 1668 . \" Whales in the sea God ' s voice obey .\" -- N . E . PRIM\n",
      "ad of some of them , that when out at sea they are afraid to mention even their\n",
      " take any till the first of May , the sea being then covered with them .\" -- CO\n",
      "h than he , Flounders round the Polar Sea .\" -- CHARLES LAMB ' S TRIUMPH OF THE\n",
      "e observed : there -- pointing to the sea -- is a green pasture where our child\n",
      "f the whale , As it floundered in the sea .\" -- ELIZABETH OAKES SMITH . \" The q\n",
      "e thousand persons living here in the sea , adding largely every year to the Na\n",
      " is right , And King of the boundless sea .\" -- WHALE SONG . CHAPTER 1 Loomings\n",
      "en , I account it high time to get to sea as soon as I can . This is my substit\n",
      " at some time or other crazy to go to sea ? Why upon your first voyage as a pas\n",
      "d ? Why did the old Persians hold the sea holy ? Why did the Greeks give it a s\n",
      "ay that I am in the habit of going to sea whenever I begin to grow hazy about t\n",
      "to have it inferred that I ever go to sea as a passenger . For to go as a passe\n",
      "hing in it . Besides , passengers get sea - sick -- grow quarrelsome -- don ' t\n",
      "something of a salt , do I ever go to sea as a Commodore , or a Captain , or a \n"
     ]
    }
   ],
   "source": [
    "text1.concordance(\"sea\") #text1 es Moby Dick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener colocaciones\n",
    "Las colocaciones son unidades fraseológicas de dos o más palabras que se usan muy habitualmente combinadas, más de lo que probabilísticamente se daría. Por ejemplo, en español \"alto riesgo\", \"sin duda\", \"no hay problema\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las colocaciones son muy específicas del tipo de texto. Por ejemplo, para obtener \"red wine\" como colocación, deberíamos procesar un texto mucho más largo y que trate sobre vinos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similitud distribucional\n",
    "Permite buscar palabras que aparecen en los mismos contextos de una palabra dada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.similar(\"sea\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras más comunes\n",
    "\n",
    "FreqDist permite obtener la distribución de frecuencias de un texto. FreqDist puede verse como un diccionario en el que la clave es la palabra y el valor es la frecuencia correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "fdist1 = FreqDist(text1)\n",
    "#imprimimos las 500 palabras más frecuentes con una longitud de al menos 5 letras\n",
    "print([i for i in fdist1.most_common(500) if len(i[0]) > 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otras funciones interesantes de FreqDist\n",
    "\n",
    " - fdist.values(): Devuelve la cantidad de veces que aparece cada palabra\n",
    " - fdist.N(): Total de palabras en el texto\n",
    " - fdist[word]: Cantidad de ocurrencias de word\n",
    " - fdist.freq(word): Devuelve la frecuencia de aparación de word (cantidad de ocurrencias / cantidad total de palabras)\n",
    " - fdist.max(): Palabra con máxima frecuencia\n",
    " - fdist.plot(): Representación gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fdist1['whales'])\n",
    "print(fdist1.N())\n",
    "print(fdist1.freq('whales'))\n",
    "x = fdist1['whales'] / fdist1.N()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge matplotlib\n",
    "%matplotlib inline \n",
    "fdist1.plot(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersión de una palabra\n",
    "Podemos también ver gráficamente como evoluciona la aparición de una palabra a lo largo del documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.dispersion_plot([\"sea\", \"whales\", \"Starbuck\",\"savage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de información del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnet\n",
    "Wordnet es un diccionario semántico, similar a un tesauro pero con una estructura más rica. NLTK incluye wordnet en inglés con 155287 palabras y 117659 conjuntos de sinónimos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synsets\n",
    "En Wordnet, los conjuntos de sinónimos se conocen como **synsets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "print(\"Synset en el que se encuentra automobile: \",wn.synsets('automobile') ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exploramos el synset car.n.01\")\n",
    "print(\"Otras palabras en el mismo synset: \",wn.synset('car.n.01').lemma_names() )       \n",
    "print(\"Definición del synset car.n.01: \",wn.synset('car.n.01').definition())\n",
    "print(\"Ejemplos de uso de la palabra: \",wn.synset('car.n.01').examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exploramos el synset automobile.v.01\")\n",
    "print(\"Otras palabras en el mismo synset: \",wn.synset('automobile.v.01').lemma_names() )       \n",
    "print(\"Definición del synset automobile.v.01: \",wn.synset('automobile.v.01').definition())\n",
    "print(\"Ejemplos de uso de la palabra: \",wn.synset('automobile.v.01').examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hipónimos e hipernónimos\n",
    "Los synsets de wordnet corresponden a conceptos abstractos, y no siempre poseen una palabra que los describan. Estos conceptos están conectados entre sí en una jerarquía.\n",
    "Los métodos para acceder a esta jerarquía son hyponyms() y hypernyms()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automobile = wn.synset('car.n.01')\n",
    "\n",
    "types_of_automobile = automobile.hyponyms()\n",
    "print(\"Tipos de automobile: \")\n",
    "for t in types_of_automobile:\n",
    "    print(t)\n",
    "\n",
    "parent_of_automobile = automobile.hypernyms()\n",
    "print(\"\\nPadres de automobile: \",parent_of_automobile)\n",
    "print(\"(\",wn.synset('motor_vehicle.n.01').definition(),\")\")\n",
    "\n",
    "paths = automobile.hypernym_paths()\n",
    "print(\"\\nTodos los \"\"ancestros\"\" de automobile son: \")\n",
    "for l in paths:\n",
    "    for w in l:\n",
    "        print(w)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similitud semántica\n",
    "\n",
    "Dado un synset particular, podemos recorrer la red de WordNet para encontrar synsets con significados similares. Saber qué palabras son semánticamente similares puede ser útil para indexar una colección de documentos, de tal forma que cuando buscamos un término general encuentre documentos con términos más específicos.\n",
    "\n",
    "Una forma de saber si un concepto es general o específico es ver la profundidad del synset. **path_similarity** asigna un valor en el rango 0-1 basado en el camino más corto que conecta el concepto en la herarquía de hypernónimos (retorna -1 si no se encuentra un camino). Comparar un synset consigo mismo retorna 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_whale = wn.synset('right_whale.n.01')\n",
    "orca = wn.synset('orca.n.01')\n",
    "tortoise = wn.synset('tortoise.n.01')\n",
    "\n",
    "print(\"Comparación semántica entre la ballena franca y la orca\")\n",
    "print(\"Hipernónimo común más cercano: \",right_whale.lowest_common_hypernyms(orca))\n",
    "print(\"Distancia mínima desde la raiz de conceptos de la orca: \",orca.min_depth())\n",
    "print(\"Distancia mínima desde la raiz de conceptos de la ballena franca: \",right_whale.min_depth())\n",
    "print(\"Similitud semántica: \",right_whale.path_similarity(orca))\n",
    "print(\"\\nComparación semántica entre la ballena franca y la tortuga\")\n",
    "print(\"Hipernónimo común más cercano: \",right_whale.lowest_common_hypernyms(tortoise))\n",
    "print(\"Distancia mínima desde la raiz de conceptos de la tortuga: \",tortoise.min_depth())\n",
    "print(\"Distancia mínima desde la raiz de conceptos de la ballena franca: \",right_whale.min_depth())\n",
    "print(\"Similitud semántica:\",right_whale.path_similarity(tortoise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speach (PoS) tagging\n",
    "\n",
    "#### Penn Treebank tagset\n",
    "![Penn Treebank tagset](penn-tagset.png \"Penn Treebank tagset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = word_tokenize(\"Mike rowed his boat in 1997\")\n",
    "pos_tagged = nltk.pos_tag(text)\n",
    "print(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.ne_chunk(pos_tagged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctico 1\n",
    "\n",
    "* Descargar los 27 capítulos del libro \"The little prince (Antoine de Saint Exupéry)\" a partir de la siguiente url: http://www.angelfire.com/hi/littleprince/frames.html\n",
    "* Identificar las stopwords particulares para este texto. Justificar.\n",
    "* Eliminar ruido, tokenizar y normalizar el texto descargado con las tareas que considere convenientes.\n",
    "* Presentar un gráfico con la dispersión de las palabras little, prince, sheep, planet, astronomer, king, rose\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
